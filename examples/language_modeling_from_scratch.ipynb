{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vaibhav803/express/blob/master/examples/language_modeling_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, Sampler\n",
        "\n",
        "import lance\n",
        "\n",
        "\n",
        "def apply_fim(sample, fim_prefix, fim_middle, fim_suffix, fim_pad, mode, np_rng):\n",
        "    \"\"\"\n",
        "    Applies FIM transformation on one sample\n",
        "    \"\"\"\n",
        "    boundaries = sorted(np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
        "\n",
        "    prefix = sample[: boundaries[0]]\n",
        "    middle = sample[boundaries[0] : boundaries[1]]\n",
        "    suffix = sample[boundaries[1] :]\n",
        "\n",
        "    total_length = len(prefix) + len(middle) + len(suffix) + 3\n",
        "    diff = total_length - len(sample)\n",
        "    if diff > 0:\n",
        "        suffix = suffix[: max(0, len(suffix) - diff)]\n",
        "    elif diff < 0:\n",
        "        extend = torch.cat([fim_pad for _ in range(-diff)])\n",
        "        suffix = torch.cat([suffix, extend])\n",
        "\n",
        "    if mode == \"spm\":\n",
        "        # Apply SPM\n",
        "        transfomed_example = torch.cat(\n",
        "            [fim_prefix, fim_suffix, suffix, fim_middle, prefix, middle]\n",
        "        )\n",
        "    else:\n",
        "        # Apply PSM\n",
        "        transfomed_example = torch.cat(\n",
        "            [fim_prefix, prefix, fim_suffix, suffix, fim_middle, middle]\n",
        "        )\n",
        "\n",
        "    return transfomed_example\n",
        "\n",
        "\n",
        "class MambaDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dataset_path,\n",
        "        context_len,\n",
        "        fim_prefix,\n",
        "        fim_middle,\n",
        "        fim_suffix,\n",
        "        fim_pad,\n",
        "        fim_rate=0.5,\n",
        "        mode=\"psm\",\n",
        "        rng_seed=42,\n",
        "    ):\n",
        "        # Load the lance dataset from the saved path\n",
        "        self.ds = lance.dataset(dataset_path)\n",
        "        self.context_len = context_len\n",
        "\n",
        "        # Doing this so the sampler never asks for an index at the end of text\n",
        "        self.length = self.ds.count_rows() - context_len\n",
        "\n",
        "        self.np_rng = np.random.RandomState(seed=rng_seed)\n",
        "\n",
        "        self.fim_prefix = torch.tensor([fim_prefix])\n",
        "        self.fim_middle = torch.tensor([fim_middle])\n",
        "        self.fim_suffix = torch.tensor([fim_suffix])\n",
        "        self.fim_pad = torch.tensor([fim_pad])\n",
        "        self.fim_rate = fim_rate\n",
        "        self.mode = mode\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def from_idxs(self, idxs):\n",
        "        \"\"\"\n",
        "        Little utility function to get the data from lance\n",
        "        \"\"\"\n",
        "        data = self.ds.take(idxs).to_pylist()\n",
        "        data = torch.tensor(list(map(lambda x: x[\"value\"], data)))\n",
        "        return data\n",
        "\n",
        "    def apply_fim(self, sample):\n",
        "        \"\"\"\n",
        "        Applies FIM transformation on one sample\n",
        "        \"\"\"\n",
        "        boundaries = sorted(self.np_rng.randint(low=0, high=len(sample) + 1, size=2))\n",
        "\n",
        "        prefix = sample[: boundaries[0]]\n",
        "        middle = sample[boundaries[0] : boundaries[1]]\n",
        "        suffix = sample[boundaries[1] :]\n",
        "\n",
        "        total_length = len(prefix) + len(middle) + len(suffix) + 3\n",
        "        diff = total_length - len(sample)\n",
        "        if diff > 0:\n",
        "            suffix = suffix[: max(0, len(suffix) - diff)]\n",
        "        elif diff < 0:\n",
        "            extend = torch.cat([self.fim_pad for _ in range(-diff)])\n",
        "            suffix = torch.cat([suffix, extend])\n",
        "\n",
        "        if self.mode == \"spm\":\n",
        "            # Apply SPM\n",
        "            transfomed_example = torch.cat(\n",
        "                [\n",
        "                    self.fim_prefix,\n",
        "                    self.fim_suffix,\n",
        "                    suffix,\n",
        "                    self.fim_middle,\n",
        "                    prefix,\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            # Apply PSM\n",
        "            transfomed_example = torch.cat(\n",
        "                [\n",
        "                    self.fim_prefix,\n",
        "                    prefix,\n",
        "                    self.fim_suffix,\n",
        "                    suffix,\n",
        "                    self.fim_middle,\n",
        "                    middle,\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        return transfomed_example\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generate a list of indices starting from the current idx to idx+context_len+1\n",
        "        with optional fim transformation\n",
        "        \"\"\"\n",
        "        current_window_idxs = np.arange(idx, idx + self.context_len + 1)\n",
        "        sample = self.from_idxs(current_window_idxs)\n",
        "\n",
        "        # Apply FIM transformation depending on the rate\n",
        "        if self.np_rng.binomial(1, self.fim_rate):\n",
        "            sample = self.apply_fim(sample)\n",
        "\n",
        "        # +1 in labels because it is 1 step ahead of input tokens\n",
        "        tokens = sample[0 : self.context_len]\n",
        "        labels = sample[1 : self.context_len + 1]\n",
        "        return {\"tokens\": tokens, \"labels\": labels}\n",
        "\n",
        "\n",
        "class MambaSampler(Sampler):\n",
        "    r\"\"\"Samples tokens randomly but `k` indices apart where `k` is generally the context length of the LLM.\n",
        "\n",
        "    Args:\n",
        "        data_source (Dataset): dataset to sample from\n",
        "        k (int): minimum index distance between each random sample\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_source, k=16):\n",
        "        self.data_source = data_source\n",
        "        self.num_samples = len(self.data_source)\n",
        "        self.available_indices = list(range(0, self.num_samples, k))\n",
        "        random.shuffle(self.available_indices)\n",
        "\n",
        "    def __iter__(self):\n",
        "        yield from self.available_indices\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.available_indices)\n"
      ],
      "metadata": {
        "id": "Fre2rZ6CpXdG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright (c) 2023, Albert Gu, Tri Dao.\n",
        "\n",
        "import math\n",
        "from functools import partial\n",
        "import json\n",
        "import os\n",
        "\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from mamba_ssm.models.config_mamba import MambaConfig\n",
        "from mamba_ssm.modules.mamba_simple import Mamba, Block\n",
        "from mamba_ssm.utils.generation import GenerationMixin\n",
        "from mamba_ssm.utils.hf import load_config_hf, load_state_dict_hf\n",
        "\n",
        "try:\n",
        "    from mamba_ssm.ops.triton.layernorm import RMSNorm, layer_norm_fn, rms_norm_fn\n",
        "except ImportError:\n",
        "    RMSNorm, layer_norm_fn, rms_norm_fn = None, None, None\n",
        "\n",
        "\n",
        "def create_block(\n",
        "    d_model,\n",
        "    ssm_cfg=None,\n",
        "    norm_epsilon=1e-5,\n",
        "    rms_norm=False,\n",
        "    residual_in_fp32=False,\n",
        "    fused_add_norm=False,\n",
        "    layer_idx=None,\n",
        "    device=None,\n",
        "    dtype=None,\n",
        "):\n",
        "    if ssm_cfg is None:\n",
        "        ssm_cfg = {}\n",
        "    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "    mixer_cls = partial(Mamba, layer_idx=layer_idx, **ssm_cfg, **factory_kwargs)\n",
        "    norm_cls = partial(\n",
        "        nn.LayerNorm if not rms_norm else RMSNorm, eps=norm_epsilon, **factory_kwargs\n",
        "    )\n",
        "    block = Block(\n",
        "        d_model,\n",
        "        mixer_cls,\n",
        "        norm_cls=norm_cls,\n",
        "        fused_add_norm=fused_add_norm,\n",
        "        residual_in_fp32=residual_in_fp32,\n",
        "    )\n",
        "    block.layer_idx = layer_idx\n",
        "    return block\n",
        "\n",
        "\n",
        "# https://github.com/huggingface/transformers/blob/c28d04e9e252a1a099944e325685f14d242ecdcd/src/transformers/models/gpt2/modeling_gpt2.py#L454\n",
        "def _init_weights(\n",
        "    module,\n",
        "    n_layer,\n",
        "    initializer_range=0.02,  # Now only used for embedding layer.\n",
        "    rescale_prenorm_residual=True,\n",
        "    n_residuals_per_layer=1,  # Change to 2 if we have MLP\n",
        "):\n",
        "    if isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "            if not getattr(module.bias, \"_no_reinit\", False):\n",
        "                nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module, nn.Embedding):\n",
        "        nn.init.normal_(module.weight, std=initializer_range)\n",
        "\n",
        "    if rescale_prenorm_residual:\n",
        "        # Reinitialize selected weights subject to the OpenAI GPT-2 Paper Scheme:\n",
        "        #   > A modified initialization which accounts for the accumulation on the residual path with model depth. Scale\n",
        "        #   > the weights of residual layers at initialization by a factor of 1/√N where N is the # of residual layers.\n",
        "        #   >   -- GPT-2 :: https://openai.com/blog/better-language-models/\n",
        "        #\n",
        "        # Reference (Megatron-LM): https://github.com/NVIDIA/Megatron-LM/blob/main/megatron/model/gpt_model.py\n",
        "        for name, p in module.named_parameters():\n",
        "            if name in [\"out_proj.weight\", \"fc2.weight\"]:\n",
        "                # Special Scaled Initialization --> There are 2 Layer Norms per Transformer Block\n",
        "                # Following Pytorch init, except scale by 1/sqrt(2 * n_layer)\n",
        "                # We need to reinit p since this code could be called multiple times\n",
        "                # Having just p *= scale would repeatedly scale it down\n",
        "                nn.init.kaiming_uniform_(p, a=math.sqrt(5))\n",
        "                with torch.no_grad():\n",
        "                    p /= math.sqrt(n_residuals_per_layer * n_layer)\n",
        "\n",
        "\n",
        "class MixerModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_layer: int,\n",
        "        vocab_size: int,\n",
        "        ssm_cfg=None,\n",
        "        norm_epsilon: float = 1e-5,\n",
        "        rms_norm: bool = False,\n",
        "        initializer_cfg=None,\n",
        "        fused_add_norm=False,\n",
        "        residual_in_fp32=False,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "        super().__init__()\n",
        "        self.residual_in_fp32 = residual_in_fp32\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model, **factory_kwargs)\n",
        "\n",
        "        # We change the order of residual and layer norm:\n",
        "        # Instead of LN -> Attn / MLP -> Add, we do:\n",
        "        # Add -> LN -> Attn / MLP / Mixer, returning both the residual branch (output of Add) and\n",
        "        # the main branch (output of MLP / Mixer). The model definition is unchanged.\n",
        "        # This is for performance reason: we can fuse add + layer_norm.\n",
        "        self.fused_add_norm = fused_add_norm\n",
        "        if self.fused_add_norm:\n",
        "            if layer_norm_fn is None or rms_norm_fn is None:\n",
        "                raise ImportError(\"Failed to import Triton LayerNorm / RMSNorm kernels\")\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [\n",
        "                create_block(\n",
        "                    d_model,\n",
        "                    ssm_cfg=ssm_cfg,\n",
        "                    norm_epsilon=norm_epsilon,\n",
        "                    rms_norm=rms_norm,\n",
        "                    residual_in_fp32=residual_in_fp32,\n",
        "                    fused_add_norm=fused_add_norm,\n",
        "                    layer_idx=i,\n",
        "                    **factory_kwargs,\n",
        "                )\n",
        "                for i in range(n_layer)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.norm_f = (nn.LayerNorm if not rms_norm else RMSNorm)(\n",
        "            d_model, eps=norm_epsilon, **factory_kwargs\n",
        "        )\n",
        "\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return {\n",
        "            i: layer.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "            for i, layer in enumerate(self.layers)\n",
        "        }\n",
        "\n",
        "    def forward(self, input_ids, inference_params=None):\n",
        "        hidden_states = self.embedding(input_ids)\n",
        "        residual = None\n",
        "        for layer in self.layers:\n",
        "            hidden_states, residual = layer(\n",
        "                hidden_states, residual, inference_params=inference_params\n",
        "            )\n",
        "        if not self.fused_add_norm:\n",
        "            residual = (hidden_states + residual) if residual is not None else hidden_states\n",
        "            hidden_states = self.norm_f(residual.to(dtype=self.norm_f.weight.dtype))\n",
        "        else:\n",
        "            # Set prenorm=False here since we don't need the residual\n",
        "            fused_add_norm_fn = rms_norm_fn if isinstance(self.norm_f, RMSNorm) else layer_norm_fn\n",
        "            hidden_states = fused_add_norm_fn(\n",
        "                hidden_states,\n",
        "                self.norm_f.weight,\n",
        "                self.norm_f.bias,\n",
        "                eps=self.norm_f.eps,\n",
        "                residual=residual,\n",
        "                prenorm=False,\n",
        "                residual_in_fp32=self.residual_in_fp32,\n",
        "            )\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class MambaLMHeadModel(nn.Module, GenerationMixin):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        config: MambaConfig,\n",
        "        initializer_cfg=None,\n",
        "        device=None,\n",
        "        dtype=None,\n",
        "    ) -> None:\n",
        "        self.config = config\n",
        "        d_model = config.d_model\n",
        "        n_layer = config.n_layer\n",
        "        vocab_size = config.vocab_size\n",
        "        ssm_cfg = config.ssm_cfg\n",
        "        rms_norm = config.rms_norm\n",
        "        residual_in_fp32 = config.residual_in_fp32\n",
        "        fused_add_norm = config.fused_add_norm\n",
        "        pad_vocab_size_multiple = config.pad_vocab_size_multiple\n",
        "        factory_kwargs = {\"device\": device, \"dtype\": dtype}\n",
        "\n",
        "        super().__init__()\n",
        "        if vocab_size % pad_vocab_size_multiple != 0:\n",
        "            vocab_size += pad_vocab_size_multiple - (vocab_size % pad_vocab_size_multiple)\n",
        "        self.backbone = MixerModel(\n",
        "            d_model=d_model,\n",
        "            n_layer=n_layer,\n",
        "            vocab_size=vocab_size,\n",
        "            ssm_cfg=ssm_cfg,\n",
        "            rms_norm=rms_norm,\n",
        "            initializer_cfg=initializer_cfg,\n",
        "            fused_add_norm=fused_add_norm,\n",
        "            residual_in_fp32=residual_in_fp32,\n",
        "            **factory_kwargs,\n",
        "        )\n",
        "        self.lm_head = nn.Linear(d_model, vocab_size, bias=False, **factory_kwargs)\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.apply(\n",
        "            partial(\n",
        "                _init_weights,\n",
        "                n_layer=n_layer,\n",
        "                **(initializer_cfg if initializer_cfg is not None else {}),\n",
        "            )\n",
        "        )\n",
        "        self.tie_weights()\n",
        "\n",
        "    def tie_weights(self):\n",
        "        self.lm_head.weight = self.backbone.embedding.weight\n",
        "\n",
        "    def allocate_inference_cache(self, batch_size, max_seqlen, dtype=None, **kwargs):\n",
        "        return self.backbone.allocate_inference_cache(batch_size, max_seqlen, dtype=dtype, **kwargs)\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.backbone.embedding\n",
        "\n",
        "    def set_input_embeddings(self, new_embeddings):\n",
        "        self.backbone.embedding = new_embeddings\n",
        "        self.tie_weights()\n",
        "\n",
        "    def resize_token_embeddings(self, vocab_size):\n",
        "        old_embeddings = self.backbone.embedding\n",
        "        old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n",
        "        new_embeddings = nn.Embedding(\n",
        "            vocab_size,\n",
        "            old_embedding_dim,\n",
        "            device=old_embeddings.weight.device,\n",
        "            dtype=old_embeddings.weight.dtype,\n",
        "        )\n",
        "        nn.init.normal_(new_embeddings.weight, std=0.02)\n",
        "        n = min(old_num_tokens, vocab_size)\n",
        "        new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n",
        "        self.backbone.embedding = new_embeddings\n",
        "\n",
        "        self.tie_weights()\n",
        "\n",
        "    def forward(self, input_ids, position_ids=None, inference_params=None, num_last_tokens=0):\n",
        "        \"\"\"\n",
        "        Changing this function from the original Mamba implementation to make it work\n",
        "        with my training scripts (-Tanay)\n",
        "\n",
        "        \"position_ids\" is just to be compatible with Transformer generation. We don't use it.\n",
        "        num_last_tokens: if > 0, only return the logits for the last n tokens\n",
        "        \"\"\"\n",
        "        hidden_states = self.backbone(input_ids, inference_params=inference_params)\n",
        "        if num_last_tokens > 0:\n",
        "            hidden_states = hidden_states[:, -num_last_tokens]\n",
        "        lm_logits = self.lm_head(hidden_states)\n",
        "        return lm_logits\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name, device=None, dtype=None, **kwargs):\n",
        "        config_data = load_config_hf(pretrained_model_name)\n",
        "        config = MambaConfig(**config_data)\n",
        "        model = cls(config, device=device, dtype=dtype, **kwargs)\n",
        "        model.load_state_dict(load_state_dict_hf(pretrained_model_name, device=device, dtype=dtype))\n",
        "        return model\n",
        "\n",
        "    def save_pretrained(self, save_directory):\n",
        "        \"\"\"\n",
        "        Minimal implementation of save_pretrained for MambaLMHeadModel.\n",
        "        Save the model and its configuration file to a directory.\n",
        "        \"\"\"\n",
        "        # Ensure save_directory exists\n",
        "        if not os.path.exists(save_directory):\n",
        "            os.makedirs(save_directory)\n",
        "\n",
        "        # Save the model's state_dict\n",
        "        model_path = os.path.join(save_directory, 'pytorch_model.bin')\n",
        "        torch.save(self.state_dict(), model_path)\n",
        "\n",
        "        # Save the configuration of the model\n",
        "        config_path = os.path.join(save_directory, 'config.json')\n",
        "        with open(config_path, 'w') as f:\n",
        "            json.dump(self.config.__dict__, f)"
      ],
      "metadata": {
        "id": "BGF6GENbposj",
        "outputId": "5e39a6b0-7970-4314-f4ab-c19ab6a925cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mamba_ssm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-172651367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_mamba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMambaConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmamba_simple\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMamba\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGenerationMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install causal-conv1d>=1.1.0"
      ],
      "metadata": {
        "id": "ZbnhK2c1qpsZ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mamba-ssm\n"
      ],
      "metadata": {
        "id": "98YPxQ-gqr82",
        "outputId": "d3cc338f-4766-4fa2-d2ac-852b1aed2ee4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (2.8.0+cu126)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (3.4.0)\n",
            "Requirement already satisfied: ninja in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (1.13.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (0.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (4.56.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (25.0)\n",
            "Requirement already satisfied: setuptools>=61.0.0 in /usr/local/lib/python3.12/dist-packages (from mamba-ssm) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->mamba-ssm) (1.11.1.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers->mamba-ssm) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers->mamba-ssm) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->mamba-ssm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers->mamba-ssm) (2025.8.3)\n",
            "Building wheels for collected packages: mamba-ssm\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mamba-ssm: filename=mamba_ssm-2.2.5-cp312-cp312-linux_x86_64.whl size=532566033 sha256=c8b65fcabfb49a94456c9971619007218e4073f19a84fb6b3894f33d43bee4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/55/c4/85b634055d6a9b599d27f5cbeacf353c6c532d8e2d8769960b\n",
            "Successfully built mamba-ssm\n",
            "Installing collected packages: mamba-ssm\n",
            "Successfully installed mamba-ssm-2.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy"
      ],
      "metadata": {
        "id": "lECkswdQqsK6",
        "outputId": "42c5cdf2-4076-4660-e7aa-b73c92b8a278",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "qg61Y5-_q0nJ",
        "outputId": "d6ac5fe3-124b-4b21-e589-773b7a520fa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "w_GSeMe9sR_d",
        "outputId": "9f39a31d-4b82-418f-fe98-a06189696a25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install wandb\n"
      ],
      "metadata": {
        "id": "OyvVdexwsT8c",
        "outputId": "f07bf98b-fa5a-486b-9141-328f3fd93227",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.21.3)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.2.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.7)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.37.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single GPU training script using FIM\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import transformers\n",
        "\n",
        "from mamba_ssm import MambaLMHeadModel\n",
        "\n",
        "import lance\n",
        "import pyarrow as pa\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "from data import MambaDataset, MambaSampler\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "# Params (replace with Arg parser later)\n",
        "class Args:\n",
        "    wandb = False\n",
        "    tokenizer_model = \"EleutherAI/gpt-neox-20b\"\n",
        "    model_name = \"state-spaces/mamba-790m\"\n",
        "    dataset_path = (\n",
        "        \"/teamspace/studios/codeparrot-dataset-lance/code_parrot_github_python.lance\"\n",
        "    )\n",
        "    eval_dataset_path = \"fim_data_eval.lance\"\n",
        "    dataset = lance.dataset(dataset_path)\n",
        "    low_cpu_mem_usage = False\n",
        "    fim_training = True\n",
        "    fim_rate = 0.9\n",
        "    truncate_or_pad = True\n",
        "    fim_prefix_token = \"<fim_prefix>\"\n",
        "    fim_middle_token = \"<fim_middle_token>\"\n",
        "    fim_suffix_token = \"<fim_suffix_token>\"\n",
        "    fim_pad_token = \"<fim_pad>\"\n",
        "    pad_factor = 8\n",
        "    lr = 1e-4\n",
        "    epochs = 10\n",
        "    context_len = 384\n",
        "    train_batch_size = 8\n",
        "    valid_batch_size = 8\n",
        "    T_0 = 1000\n",
        "    T_mult = 1\n",
        "    eta_min = 1e-5\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    # Total chunks of context_len+1 size we can get\n",
        "    steps_per_epoch = (dataset.count_rows() // context_len + 1) // 4\n",
        "\n",
        "\n",
        "# Define Tokenizer and Model\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(Args.tokenizer_model)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = MambaLMHeadModel.from_pretrained(\n",
        "    Args.model_name,\n",
        ").to(Args.device)\n",
        "\n",
        "# Get the FIM-specific tokens and get their token ids\n",
        "tokenizer.add_tokens(\n",
        "    [\n",
        "        Args.fim_prefix_token,\n",
        "        Args.fim_middle_token,\n",
        "        Args.fim_middle_token,\n",
        "        Args.fim_pad_token,\n",
        "    ]\n",
        ")\n",
        "prefix_tok_id = tokenizer.convert_tokens_to_ids(Args.fim_prefix_token)\n",
        "middle_tok_id = tokenizer.convert_tokens_to_ids(Args.fim_middle_token)\n",
        "suffix_tok_id = tokenizer.convert_tokens_to_ids(Args.fim_middle_token)\n",
        "pad_tok_id = None\n",
        "\n",
        "fim_tokens = [prefix_tok_id, middle_tok_id, suffix_tok_id]\n",
        "\n",
        "# If truncate_or_pad is on, also get pad token id\n",
        "if Args.truncate_or_pad:\n",
        "    pad_tok_id = tokenizer.convert_tokens_to_ids(Args.fim_pad_token)\n",
        "    fim_tokens.append(pad_tok_id)\n",
        "\n",
        "# Add new tokens and resize model token embeddings according to multivariate normal distribution\n",
        "original_embeddings = model.get_input_embeddings().weight\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "mean = original_embeddings.mean(dim=0)\n",
        "n = original_embeddings.size()[0]\n",
        "sigma = ((original_embeddings - mean).T @ (original_embeddings - mean)) / n\n",
        "dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=1e-5 * sigma)\n",
        "new_token_embeddings = torch.stack(\n",
        "    tuple((dist.sample() for _ in range(len(fim_tokens)))), dim=0\n",
        ")\n",
        "\n",
        "# Get updated embedding layer and make a copy of it's weights\n",
        "embeddings = model.get_input_embeddings()\n",
        "new_embeddings = embeddings.weight.clone()\n",
        "\n",
        "# Set the new token' embeddings to the newly sampled embeddings\n",
        "new_embeddings[-len(fim_tokens) :] = new_token_embeddings\n",
        "\n",
        "# Update the model's embeddings with the new embeddings\n",
        "embeddings.weight = torch.nn.Parameter(new_embeddings)\n",
        "model.set_input_embeddings(embeddings)\n",
        "\n",
        "# Make train dataset and train dataloader\n",
        "train_dataset = MambaDataset(\n",
        "    Args.dataset_path,\n",
        "    context_len=Args.context_len,\n",
        "    fim_prefix=prefix_tok_id,\n",
        "    fim_middle=middle_tok_id,\n",
        "    fim_suffix=suffix_tok_id,\n",
        "    fim_pad=pad_tok_id,\n",
        "    fim_rate=Args.fim_rate,\n",
        "    mode=\"psm\",\n",
        ")\n",
        "\n",
        "train_dataloader = iter(\n",
        "    DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=Args.train_batch_size,\n",
        "        sampler=MambaSampler(train_dataset, k=Args.context_len + 1),\n",
        "        shuffle=False,\n",
        "        pin_memory=True,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Optimizer and Scheduler\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=Args.lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer, T_0=Args.T_0, T_mult=Args.T_mult, eta_min=Args.eta_min\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(f\"{'*'*8} Starting training {'*'*8}\")\n",
        "print(f\"Total training tokens: {lance.dataset(Args.dataset_path).count_rows():,}\")\n",
        "print(f\"Epochs to train: {Args.epochs}\")\n",
        "print(f\"Training steps per epoch: {Args.steps_per_epoch:,}\\n\")\n",
        "# print(f\"Total training steps in training: {Args.steps_per_epoch * Args.epochs:,}\")\n",
        "\n",
        "\n",
        "def wandb_log(**kwargs):\n",
        "    \"\"\"Easy interface to log stuff to wandb\"\"\"\n",
        "    for k, v in kwargs.items():\n",
        "        wandb.log({k: v})\n",
        "\n",
        "\n",
        "if Args.wandb:\n",
        "    # Convert the Config class to a dict for logging\n",
        "    config_dict = dict(vars(Args))\n",
        "    del [config_dict[\"__module__\"]]\n",
        "    del [config_dict[\"__dict__\"]]\n",
        "    del [config_dict[\"__weakref__\"]]\n",
        "    del [config_dict[\"__doc__\"]]\n",
        "\n",
        "    from dotenv import load_dotenv\n",
        "\n",
        "    load_dotenv()\n",
        "    wandb.login()\n",
        "    run = wandb.init(\n",
        "        project=\"pytorch\",\n",
        "        config=config_dict,\n",
        "        group=\"mamba-train\",\n",
        "        job_type=\"train\",\n",
        "    )\n",
        "    wandb.watch(model)\n",
        "\n",
        "prog_bar = tqdm(\n",
        "    range(Args.steps_per_epoch * Args.epochs), total=Args.steps_per_epoch * Args.epochs\n",
        ")\n",
        "for epoch in range(Args.epochs):\n",
        "    model.train()\n",
        "    total_loss = []\n",
        "    for step in range(Args.steps_per_epoch):\n",
        "        # Get the next batch\n",
        "        batch = next(train_dataloader)\n",
        "        for k, v in batch.items():\n",
        "            batch[k] = v.to(Args.device)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model(batch[\"tokens\"])\n",
        "\n",
        "        # Reshape predictions and calculate loss\n",
        "        B, C, V = predictions.shape\n",
        "        predictions = predictions.view(B * C, V)\n",
        "        targets = batch[\"labels\"].view(B * C)\n",
        "        loss = torch.nn.functional.cross_entropy(predictions, targets)\n",
        "        prog_bar.set_description((f\"loss: {loss.item():.4f}\"))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        prog_bar.update(1)\n",
        "\n",
        "        total_loss.append(loss.item())\n",
        "        if Args.wandb:\n",
        "            wandb_log(step_loss=loss.item())\n",
        "\n",
        "    # Calculate perplexity for the epoch\n",
        "    try:\n",
        "        perplexity = np.exp(np.mean(total_loss))\n",
        "    except OverflowError:\n",
        "        perplexity = float(\"-inf\")\n",
        "\n",
        "    if Args.wandb:\n",
        "        wandb_log(train_perplexity=perplexity)\n",
        "\n",
        "    print(f\"epoch: {epoch} | train perplexity: {perplexity:.4f}\")\n",
        "\n",
        "# Save the model after training\n",
        "model_name = Args.model_name.split(\"/\")[-1]\n",
        "torch.save(model.state_dict(), f\"{model_name}-fim.bin\")\n",
        "print(\"Saved the model!\")\n"
      ],
      "metadata": {
        "id": "mYsUU__isfxN",
        "outputId": "a91c1601-a96b-4b07-91de-52ec89c36628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'mamba_ssm'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3136408842.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmamba_ssm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMambaLMHeadModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mamba_ssm'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mamba_ssm\n"
      ],
      "metadata": {
        "id": "ZmANMP3_-mvH",
        "outputId": "0a1f1898-df99-481a-8f7b-c38e0a8412ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mamba_ssm\n",
            "  Downloading mamba_ssm-2.2.5.tar.gz (113 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m112.6/113.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.8/113.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install causal-conv1d>=1.4.0"
      ],
      "metadata": {
        "id": "yoTfZNW4Psli"
      },
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Train a language model",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}